{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-22T14:08:29.532045Z",
     "start_time": "2024-08-22T14:08:29.102418Z"
    }
   },
   "outputs": [],
   "source": [
    "from include import neural_net as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weathermob_data = torch.from_numpy(pd.read_csv(\"../../data/Covid_mob/Berlin_data/weather_mobility.csv\", index_col=0).to_numpy(dtype=float)).float().reshape(-1, 2, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T14:12:01.271433Z",
     "start_time": "2024-08-22T14:12:01.253500Z"
    }
   },
   "id": "746b0583238dfc8d",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[6.8000],\n         [2.0000]],\n\n        [[6.8000],\n         [2.0000]],\n\n        [[6.8000],\n         [2.0000]],\n\n        [[6.8000],\n         [2.0000]],\n\n        [[6.8000],\n         [2.0000]],\n\n        [[6.8000],\n         [2.0000]],\n\n        [[5.1700],\n         [2.0000]],\n\n        [[5.1700],\n         [2.0000]],\n\n        [[5.1700],\n         [2.0000]],\n\n        [[5.1700],\n         [2.0000]],\n\n        [[5.1700],\n         [2.0000]],\n\n        [[5.1700],\n         [2.0000]],\n\n        [[5.1700],\n         [2.0000]],\n\n        [[4.3600],\n         [2.0000]],\n\n        [[4.3600],\n         [2.0000]],\n\n        [[4.3600],\n         [2.0000]],\n\n        [[4.3600],\n         [2.0000]],\n\n        [[4.3600],\n         [2.0000]],\n\n        [[4.3600],\n         [2.0000]],\n\n        [[4.3600],\n         [2.0000]],\n\n        [[4.6700],\n         [1.8377]],\n\n        [[4.6700],\n         [1.8377]],\n\n        [[4.6700],\n         [1.8377]],\n\n        [[4.6700],\n         [1.8377]],\n\n        [[4.6700],\n         [1.8377]],\n\n        [[4.6700],\n         [1.8377]],\n\n        [[4.6700],\n         [1.8377]],\n\n        [[4.9000],\n         [1.6321]],\n\n        [[4.9000],\n         [1.6321]],\n\n        [[4.9000],\n         [1.6321]],\n\n        [[4.9000],\n         [1.6321]],\n\n        [[4.9000],\n         [1.6321]],\n\n        [[4.9000],\n         [1.6321]],\n\n        [[4.9000],\n         [1.6321]],\n\n        [[4.7600],\n         [1.7327]],\n\n        [[4.7600],\n         [1.7327]],\n\n        [[4.7600],\n         [1.7327]],\n\n        [[4.7600],\n         [1.7327]],\n\n        [[4.7600],\n         [1.7327]],\n\n        [[4.7600],\n         [1.7327]],\n\n        [[4.7600],\n         [1.7327]],\n\n        [[5.3000],\n         [1.3925]],\n\n        [[5.3000],\n         [1.3925]],\n\n        [[5.3000],\n         [1.3925]],\n\n        [[5.3000],\n         [1.3925]],\n\n        [[5.3000],\n         [1.3925]],\n\n        [[5.3000],\n         [1.3925]],\n\n        [[5.3000],\n         [1.3925]],\n\n        [[5.5800],\n         [1.8659]],\n\n        [[5.5800],\n         [1.8659]],\n\n        [[5.5800],\n         [1.8659]],\n\n        [[5.5800],\n         [1.8659]],\n\n        [[5.5800],\n         [1.8659]],\n\n        [[5.5800],\n         [1.8659]],\n\n        [[5.5800],\n         [1.8659]],\n\n        [[5.9000],\n         [1.6141]],\n\n        [[5.9000],\n         [1.6141]],\n\n        [[5.9000],\n         [1.6141]],\n\n        [[5.9000],\n         [1.6141]],\n\n        [[5.9000],\n         [1.6141]],\n\n        [[5.9000],\n         [1.6141]],\n\n        [[5.9000],\n         [1.6141]],\n\n        [[5.7700],\n         [1.7269]],\n\n        [[5.7700],\n         [1.7269]],\n\n        [[5.7700],\n         [1.7269]],\n\n        [[5.7700],\n         [1.7269]],\n\n        [[5.7700],\n         [1.7269]],\n\n        [[5.7700],\n         [1.7269]],\n\n        [[5.7700],\n         [1.7269]],\n\n        [[5.8500],\n         [1.5364]],\n\n        [[5.8500],\n         [1.5364]],\n\n        [[5.8500],\n         [1.5364]],\n\n        [[5.8500],\n         [1.5364]],\n\n        [[5.8500],\n         [1.5364]],\n\n        [[5.8500],\n         [1.5364]],\n\n        [[5.8500],\n         [1.5364]],\n\n        [[6.4400],\n         [1.3063]],\n\n        [[6.4400],\n         [1.3063]],\n\n        [[6.4400],\n         [1.3063]],\n\n        [[6.4400],\n         [1.3063]],\n\n        [[6.4400],\n         [1.3063]],\n\n        [[6.4400],\n         [1.3063]],\n\n        [[6.4400],\n         [1.3063]],\n\n        [[6.2800],\n         [1.3879]],\n\n        [[6.2800],\n         [1.3879]],\n\n        [[6.2800],\n         [1.3879]],\n\n        [[6.2800],\n         [1.3879]],\n\n        [[6.2800],\n         [1.3879]],\n\n        [[6.2800],\n         [1.3879]],\n\n        [[6.2800],\n         [1.3879]],\n\n        [[6.9200],\n         [1.0341]],\n\n        [[6.9200],\n         [1.0341]],\n\n        [[6.9200],\n         [1.0341]],\n\n        [[6.9200],\n         [1.0341]],\n\n        [[6.9200],\n         [1.0341]],\n\n        [[6.9200],\n         [1.0341]],\n\n        [[6.9200],\n         [1.0341]],\n\n        [[7.2600],\n         [1.1932]],\n\n        [[7.2600],\n         [1.1932]],\n\n        [[7.2600],\n         [1.1932]],\n\n        [[7.2600],\n         [1.1932]],\n\n        [[7.2600],\n         [1.1932]],\n\n        [[7.2600],\n         [1.1932]],\n\n        [[7.2600],\n         [1.1932]],\n\n        [[7.3300],\n         [1.0000]],\n\n        [[7.3300],\n         [1.0000]],\n\n        [[7.3300],\n         [1.0000]],\n\n        [[7.3300],\n         [1.0000]],\n\n        [[7.3300],\n         [1.0000]],\n\n        [[7.3300],\n         [1.0000]],\n\n        [[7.3300],\n         [1.0000]],\n\n        [[6.8500],\n         [1.2406]],\n\n        [[6.8500],\n         [1.2406]],\n\n        [[6.8500],\n         [1.2406]],\n\n        [[6.8500],\n         [1.2406]],\n\n        [[6.8500],\n         [1.2406]],\n\n        [[6.8500],\n         [1.2406]],\n\n        [[6.8500],\n         [1.2406]],\n\n        [[6.7100],\n         [1.4296]],\n\n        [[6.7100],\n         [1.4296]],\n\n        [[6.7100],\n         [1.4296]],\n\n        [[6.7100],\n         [1.4296]],\n\n        [[6.7100],\n         [1.4296]],\n\n        [[6.7100],\n         [1.4296]],\n\n        [[6.7100],\n         [1.4296]],\n\n        [[6.9400],\n         [1.3411]],\n\n        [[6.9400],\n         [1.3411]],\n\n        [[6.9400],\n         [1.3411]],\n\n        [[6.9400],\n         [1.3411]],\n\n        [[6.9400],\n         [1.3411]],\n\n        [[6.9400],\n         [1.3411]],\n\n        [[6.9400],\n         [1.3411]],\n\n        [[6.8800],\n         [1.2118]],\n\n        [[6.8800],\n         [1.2118]],\n\n        [[6.8800],\n         [1.2118]],\n\n        [[6.8800],\n         [1.2118]],\n\n        [[6.8800],\n         [1.2118]],\n\n        [[6.8800],\n         [1.2118]],\n\n        [[6.8800],\n         [1.2118]],\n\n        [[6.9700],\n         [1.2743]],\n\n        [[6.9700],\n         [1.2743]],\n\n        [[6.9700],\n         [1.2743]],\n\n        [[6.9700],\n         [1.2743]],\n\n        [[6.9700],\n         [1.2743]],\n\n        [[6.9700],\n         [1.2743]],\n\n        [[6.9700],\n         [1.2743]],\n\n        [[7.1500],\n         [1.0000]],\n\n        [[7.1500],\n         [1.0000]],\n\n        [[7.1500],\n         [1.0000]],\n\n        [[7.1500],\n         [1.0000]],\n\n        [[7.1500],\n         [1.0000]],\n\n        [[7.1500],\n         [1.0000]],\n\n        [[7.1500],\n         [1.0000]],\n\n        [[7.2200],\n         [1.0000]],\n\n        [[7.2200],\n         [1.0000]],\n\n        [[7.2200],\n         [1.0000]],\n\n        [[7.2200],\n         [1.0000]],\n\n        [[7.2200],\n         [1.0000]],\n\n        [[7.2200],\n         [1.0000]],\n\n        [[7.2200],\n         [1.0000]],\n\n        [[7.1500],\n         [1.0794]],\n\n        [[7.1500],\n         [1.0794]],\n\n        [[7.1500],\n         [1.0794]],\n\n        [[7.1500],\n         [1.0794]],\n\n        [[7.1500],\n         [1.0794]],\n\n        [[7.1500],\n         [1.0794]],\n\n        [[7.1500],\n         [1.0794]],\n\n        [[7.3100],\n         [1.6562]],\n\n        [[7.3100],\n         [1.6562]],\n\n        [[7.3100],\n         [1.6562]],\n\n        [[7.3100],\n         [1.6562]],\n\n        [[7.3100],\n         [1.6562]],\n\n        [[7.3100],\n         [1.6562]],\n\n        [[7.3100],\n         [1.6562]],\n\n        [[7.2300],\n         [1.8004]],\n\n        [[7.2300],\n         [1.8004]],\n\n        [[7.2300],\n         [1.8004]],\n\n        [[7.2300],\n         [1.8004]],\n\n        [[7.2300],\n         [1.8004]],\n\n        [[7.2300],\n         [1.8004]],\n\n        [[7.2300],\n         [1.8004]],\n\n        [[7.7300],\n         [1.3594]],\n\n        [[7.7300],\n         [1.3594]],\n\n        [[7.7300],\n         [1.3594]],\n\n        [[7.7300],\n         [1.3594]],\n\n        [[7.7300],\n         [1.3594]],\n\n        [[7.7300],\n         [1.3594]],\n\n        [[7.7300],\n         [1.3594]],\n\n        [[7.9100],\n         [1.5566]],\n\n        [[7.9100],\n         [1.5566]],\n\n        [[7.9100],\n         [1.5566]],\n\n        [[7.9100],\n         [1.5566]],\n\n        [[7.9100],\n         [1.5566]],\n\n        [[7.9100],\n         [1.5566]],\n\n        [[7.9100],\n         [1.5566]],\n\n        [[7.3000],\n         [2.0000]],\n\n        [[7.3000],\n         [2.0000]],\n\n        [[7.3000],\n         [2.0000]],\n\n        [[7.3000],\n         [2.0000]],\n\n        [[7.3000],\n         [2.0000]],\n\n        [[7.3000],\n         [2.0000]],\n\n        [[7.3000],\n         [2.0000]],\n\n        [[7.2500],\n         [2.0000]],\n\n        [[7.2500],\n         [2.0000]],\n\n        [[7.2500],\n         [2.0000]],\n\n        [[7.2500],\n         [2.0000]],\n\n        [[7.2500],\n         [2.0000]],\n\n        [[7.2500],\n         [2.0000]],\n\n        [[7.2500],\n         [2.0000]],\n\n        [[7.1100],\n         [2.0000]],\n\n        [[7.1100],\n         [2.0000]],\n\n        [[7.1100],\n         [2.0000]],\n\n        [[7.1100],\n         [2.0000]],\n\n        [[7.1100],\n         [2.0000]],\n\n        [[7.1100],\n         [2.0000]],\n\n        [[7.1100],\n         [2.0000]],\n\n        [[6.3400],\n         [2.0000]],\n\n        [[6.3400],\n         [2.0000]],\n\n        [[6.3400],\n         [2.0000]],\n\n        [[6.3400],\n         [2.0000]],\n\n        [[6.3400],\n         [2.0000]],\n\n        [[6.3400],\n         [2.0000]],\n\n        [[6.3400],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.3700],\n         [2.0000]],\n\n        [[6.2200],\n         [2.0000]],\n\n        [[6.2200],\n         [2.0000]],\n\n        [[6.2200],\n         [2.0000]],\n\n        [[6.2200],\n         [2.0000]],\n\n        [[6.2200],\n         [2.0000]],\n\n        [[6.2200],\n         [2.0000]],\n\n        [[6.2200],\n         [2.0000]],\n\n        [[6.2900],\n         [2.0000]],\n\n        [[6.2900],\n         [2.0000]],\n\n        [[6.2900],\n         [2.0000]],\n\n        [[6.2900],\n         [2.0000]],\n\n        [[6.2900],\n         [2.0000]],\n\n        [[6.2900],\n         [2.0000]],\n\n        [[6.2900],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[5.8400],\n         [2.0000]],\n\n        [[6.5600],\n         [2.0000]],\n\n        [[6.5600],\n         [2.0000]],\n\n        [[6.5600],\n         [2.0000]],\n\n        [[6.5600],\n         [2.0000]],\n\n        [[6.5600],\n         [2.0000]],\n\n        [[6.5600],\n         [2.0000]],\n\n        [[6.5600],\n         [2.0000]],\n\n        [[6.4000],\n         [2.0000]],\n\n        [[6.4000],\n         [2.0000]],\n\n        [[6.4000],\n         [2.0000]],\n\n        [[6.4000],\n         [2.0000]],\n\n        [[6.4000],\n         [2.0000]],\n\n        [[6.4000],\n         [2.0000]],\n\n        [[6.4000],\n         [2.0000]],\n\n        [[5.5700],\n         [2.0000]],\n\n        [[5.5700],\n         [2.0000]],\n\n        [[5.5700],\n         [2.0000]],\n\n        [[5.5700],\n         [2.0000]],\n\n        [[5.5700],\n         [2.0000]],\n\n        [[5.5700],\n         [2.0000]],\n\n        [[5.5700],\n         [2.0000]],\n\n        [[4.6800],\n         [2.0000]],\n\n        [[4.6800],\n         [2.0000]],\n\n        [[4.6800],\n         [2.0000]],\n\n        [[4.6800],\n         [2.0000]],\n\n        [[4.6800],\n         [2.0000]],\n\n        [[4.6800],\n         [2.0000]],\n\n        [[4.6800],\n         [2.0000]]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weathermob_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T14:12:34.855241Z",
     "start_time": "2024-08-22T14:12:34.836050Z"
    }
   },
   "id": "283e7cfc7d6450b0",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([6.8000])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weathermob_data[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T14:13:04.149716Z",
     "start_time": "2024-08-22T14:13:04.144624Z"
    }
   },
   "id": "395b739ef9f591b4",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Covid_NN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        rng: np.random.Generator,\n",
    "        h5group: h5.Group,\n",
    "        neural_net: base.NeuralNet,\n",
    "        loss_function: dict,\n",
    "        to_learn: list,\n",
    "        time_dependent_parameters: dict = None,\n",
    "        true_parameters: dict = {},\n",
    "        dt: float,\n",
    "        k_q: float = 10.25,\n",
    "        Berlin_data_loss: bool = False,\n",
    "        write_every: int = 1,\n",
    "        write_start: int = 1,\n",
    "        training_data: torch.Tensor,\n",
    "        weathermob_data: torch.Tensor,\n",
    "        batch_size: int,\n",
    "        scaling_factors: dict = {},\n",
    "        **__,\n",
    "    ):\n",
    "        \"\"\"Initialize the model instance with a previously constructed RNG and\n",
    "        HDF5 group to write the output data to.\n",
    "\n",
    "        Args:\n",
    "            rng (np.random.Generator): The shared RNG\n",
    "            h5group (h5.Group): The output file group to write data to\n",
    "            neural_net: The neural network\n",
    "            loss_function (dict): the loss function to use\n",
    "            to_learn: the list of parameter names to learn\n",
    "            time_dependent_parameters: dictionary of time-dependent parameters and their granularity\n",
    "            true_parameters: the dictionary of true parameters\n",
    "            dt: time differential\n",
    "            k_q: contact tracing rate\n",
    "            Berlin_data_loss: whether to use the loss structure unique to the Berlin data\n",
    "            write_every: write every iteration\n",
    "            write_start: iteration at which to start writing\n",
    "            training_data: the training data to use\n",
    "            batch_size: epoch batch size: instead of calculating the entire time series,\n",
    "                only a subsample of length batch_size can be used. The likelihood is then\n",
    "                scaled up accordingly.\n",
    "            scaling_factors: dictionary of scaling factors for the different parameters. Parameter estimates are\n",
    "                multiplied by these to ensure all parameters are roughly of the same order of magnitude\n",
    "        \"\"\"\n",
    "        self._rng = rng\n",
    "\n",
    "        self.neural_net = nn\n",
    "        self.neural_net.optimizer.zero_grad()\n",
    "        self.loss_function = base.LOSS_FUNCTIONS[loss_function.get(\"name\").lower()](\n",
    "            loss_function.get(\"args\", None), **loss_function.get(\"kwargs\", {})\n",
    "        )\n",
    "\n",
    "        self.dt = torch.tensor(dt, dtype=torch.float)\n",
    "        self.k_q = torch.tensor(k_q, dtype=torch.float)\n",
    "        self.Berlin_data_loss = Berlin_data_loss\n",
    "\n",
    "        self.current_loss = torch.tensor(0.0)\n",
    "\n",
    "        self.to_learn = {key: idx for idx, key in enumerate(to_learn)}\n",
    "        self.time_dependent_parameters = (\n",
    "            time_dependent_parameters if time_dependent_parameters else {}\n",
    "        )\n",
    "        self.true_parameters = {\n",
    "            key: torch.tensor(val, dtype=torch.float)\n",
    "            for key, val in true_parameters.items()\n",
    "        }\n",
    "        self.all_parameters = set(self.to_learn.keys())\n",
    "        self.all_parameters.update(self.true_parameters.keys())\n",
    "        self.current_predictions = torch.zeros(len(self.to_learn), dtype=torch.float)\n",
    "\n",
    "        # Training data\n",
    "        self.training_data = training_data\n",
    "\n",
    "        #Weather and mobility data\n",
    "        self.weathermob_data = weathermob_data\n",
    "\n",
    "        # Generate the batch ids\n",
    "        batches = np.arange(0, self.training_data.shape[0], batch_size)\n",
    "        if len(batches) == 1:\n",
    "            batches = np.append(batches, training_data.shape[0] - 1)\n",
    "        else:\n",
    "            if batches[-1] != training_data.shape[0] - 1:\n",
    "                batches = np.append(batches, training_data.shape[0] - 1)\n",
    "\n",
    "        self.batches = batches\n",
    "\n",
    "        # --- Set up chunked dataset to store the state data in --------------------------------------------------------\n",
    "        self._dset_loss = self._h5group.create_dataset(\n",
    "            \"loss\",\n",
    "            (0,),\n",
    "            maxshape=(None,),\n",
    "            chunks=True,\n",
    "            compression=3,\n",
    "        )\n",
    "        self._dset_loss.attrs[\"dim_names\"] = [\"batch\"]\n",
    "        self._dset_loss.attrs[\"coords_mode__batch\"] = \"start_and_step\"\n",
    "        self._dset_loss.attrs[\"coords__batch\"] = [write_start, write_every]\n",
    "\n",
    "        self.dset_time = self._h5group.create_dataset(\n",
    "            \"computation_time\",\n",
    "            (0,),\n",
    "            maxshape=(None,),\n",
    "            chunks=True,\n",
    "            compression=3,\n",
    "        )\n",
    "        self.dset_time.attrs[\"dim_names\"] = [\"epoch\"]\n",
    "        self.dset_time.attrs[\"coords_mode__epoch\"] = \"trivial\"\n",
    "\n",
    "        # Create a dataset for the parameter estimates\n",
    "        self.dset_parameters = self._h5group.create_dataset(\n",
    "            \"parameters\",\n",
    "            (0, len(self.to_learn.keys())),\n",
    "            maxshape=(None, len(self.to_learn.keys())),\n",
    "            chunks=True,\n",
    "            compression=3,\n",
    "        )\n",
    "        self.dset_parameters.attrs[\"dim_names\"] = [\"batch\", \"parameter\"]\n",
    "        self.dset_parameters.attrs[\"coords_mode__batch\"] = \"start_and_step\"\n",
    "        self.dset_parameters.attrs[\"coords__batch\"] = [write_start, write_every]\n",
    "        self.dset_parameters.attrs[\"coords_mode__parameter\"] = \"values\"\n",
    "        self.dset_parameters.attrs[\"coords__parameter\"] = to_learn\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------------------------\n",
    "        # Batches processed\n",
    "        self._time = 0\n",
    "        self._write_every = write_every\n",
    "        self._write_start = write_start\n",
    "\n",
    "        # Calculate the coefficients of each term in the loss function:\n",
    "        # \\alpha_i^{-1} = \\int T_i(t) dt\n",
    "        alpha = torch.sum(training_data, dim=0) * self.dt\n",
    "        alpha = torch.where(alpha > 0, alpha, torch.tensor(1.0))\n",
    "        self.alpha = (\n",
    "            torch.cat([alpha[0:8], torch.sum(alpha[8:12], 0, keepdim=True)], 0)\n",
    "        ) ** (-1)\n",
    "\n",
    "        # Reduced data model\n",
    "        for idx in [1, 3, 4, 5, 6, 7, 8]:  # E, R, Sy, H, C, qS, qE, qI are dropped\n",
    "            self.alpha[idx] = 0\n",
    "\n",
    "        # Get all the jump points\n",
    "        self.jump_points = {}\n",
    "        if self.time_dependent_parameters:\n",
    "            self.jump_points = set(\n",
    "                np.hstack(\n",
    "                    [\n",
    "                        np.array(interval).flatten()\n",
    "                        for _, interval in self.time_dependent_parameters.items()\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            if None in self.jump_points:\n",
    "                self.jump_points.remove(None)\n",
    "\n",
    "        # Get the scaling factors\n",
    "        self.scaling_factors = torch.tensor(\n",
    "            list(\n",
    "                {\n",
    "                    key: torch.tensor(scaling_factors[key], dtype=torch.float)\n",
    "                    if key in scaling_factors.keys()\n",
    "                    else torch.tensor(1.0, dtype=torch.float)\n",
    "                    for key in self.to_learn.keys()\n",
    "                }.values()\n",
    "            ),\n",
    "            dtype=torch.float,\n",
    "        )\n",
    "\n",
    "    def epoch(self):\n",
    "        \"\"\"Trains the model for a single epoch\"\"\"\n",
    "\n",
    "        # Process the training data in batches\n",
    "        for batch_no, batch_idx in enumerate(self.batches[:-1]):\n",
    "            # Make a prediction\n",
    "            predicted_parameters = self.neural_net(\n",
    "                torch.flatten(self.training_data[batch_idx])\n",
    "            )\n",
    "\n",
    "            # Combine the predicted and true parameters into a dictionary\n",
    "            parameters = {\n",
    "                p: predicted_parameters[self.to_learn[p]]\n",
    "                * self.scaling_factors[self.to_learn[p]]\n",
    "                if p in self.to_learn.keys()\n",
    "                else self.true_parameters[p]\n",
    "                for p in self.all_parameters\n",
    "            }\n",
    "\n",
    "            # Get the initial values\n",
    "            current_densities = self.training_data[batch_idx].clone()\n",
    "            current_densities.requires_grad_(True)\n",
    "            densities = [current_densities]\n",
    "            print(\"let me print something for gods sake\")\n",
    "            print(predicted_parameters)\n",
    "            # Integrate the ODE for B steps\n",
    "            for ele in range(batch_idx + 1, self.batches[batch_no + 1] + 1):\n",
    "                # Adjust for time-dependency\n",
    "                for key, ranges in self.time_dependent_parameters.items():\n",
    "                    for idx, r in enumerate(ranges):\n",
    "                        if not r[1]:\n",
    "                            r[1] = len(self.training_data) + 1\n",
    "                        if r[0] <= ele < r[1]:\n",
    "                            parameters[key] = parameters[key + f\"_{idx}\"]\n",
    "                            break\n",
    "\n",
    "                # Calculate the k_Q parameter from the current CT figures and k_CT estimate\n",
    "                k_Q = self.k_q * parameters[\"k_CT\"] * densities[-1][-1]\n",
    "             #   print(\"k_Q is equaaaaal tooooo\", k_Q)\n",
    "\n",
    "                # Calculate the k_E parameter from weather and mobility data\n",
    "                k_E = parameters[\"k_E0\"] + parameters[\"k_E1\"] * self.weathermob_data[-1][0]**2 + parameters[\"k_E2\"] * self.weathermob_data[-1][0]**2 * self.weathermob_data[-1][1]\n",
    "             #   k_E = parameters[\"k_E\"] * self.weathermob_data[-1][1]\n",
    "              #  print(\"k_E is equaaaaal tooooo\", k_E)\n",
    "                # Solve the ODE\n",
    "                densities.append(\n",
    "                    torch.clip(\n",
    "                        densities[-1]\n",
    "                        + torch.stack(\n",
    "                            [\n",
    "                                (-k_E * densities[-1][2] - k_Q)\n",
    "                                * densities[-1][0]\n",
    "                                + parameters[\"k_S\"] * densities[-1][8],\n",
    "                                k_E * densities[-1][0] * densities[-1][2]\n",
    "                                - (parameters[\"k_I\"] + k_Q) * densities[-1][1],\n",
    "                                parameters[\"k_I\"] * densities[-1][1]\n",
    "                                - (parameters[\"k_R\"] + parameters[\"k_SY\"] + k_Q)\n",
    "                                * densities[-1][2],\n",
    "                                parameters[\"k_R\"]\n",
    "                                * (\n",
    "                                    densities[-1][2]\n",
    "                                    + densities[-1][4]\n",
    "                                    + densities[-1][5]\n",
    "                                    + densities[-1][6]\n",
    "                                    + densities[-1][10]\n",
    "                                ),\n",
    "                                parameters[\"k_SY\"]\n",
    "                                * (densities[-1][2] + densities[-1][10])\n",
    "                                - (parameters[\"k_R\"] + parameters[\"k_H\"])\n",
    "                                * densities[-1][4],\n",
    "                                parameters[\"k_H\"] * densities[-1][4]\n",
    "                                - (parameters[\"k_R\"] + parameters[\"k_C\"])\n",
    "                                * densities[-1][5],\n",
    "                                parameters[\"k_C\"] * densities[-1][5]\n",
    "                                - (parameters[\"k_R\"] + parameters[\"k_D\"])\n",
    "                                * densities[-1][6],\n",
    "                                parameters[\"k_D\"] * densities[-1][6],\n",
    "                                -parameters[\"k_S\"] * densities[-1][8]\n",
    "                                + k_Q * densities[-1][0],\n",
    "                                -parameters[\"k_I\"] * densities[-1][9]\n",
    "                                + k_Q * densities[-1][1],\n",
    "                                parameters[\"k_I\"] * densities[-1][9]\n",
    "                                + k_Q * densities[-1][2]\n",
    "                                - (parameters[\"k_SY\"] + parameters[\"k_R\"])\n",
    "                                * densities[-1][10],\n",
    "                                parameters[\"k_SY\"] * densities[-1][2]\n",
    "                                - self.k_q\n",
    "                                * torch.sum(densities[-1][0:3])\n",
    "                                * densities[-1][-1],\n",
    "                            ]\n",
    "                        )\n",
    "                        * self.dt,\n",
    "                        0,\n",
    "                        1,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Discard the initial condition\n",
    "            densities = torch.stack(densities[1:])\n",
    "\n",
    "            if self.Berlin_data_loss:\n",
    "                # For the Berlin dataset, combine the quarantine compartments and drop the deceased compartment,\n",
    "                # which is not present in the ABM data\n",
    "                densities = torch.cat(\n",
    "                    [\n",
    "                        densities[:, :8],\n",
    "                        torch.sum(densities[:, 8:12], dim=1, keepdim=True),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                loss = (\n",
    "                    self.alpha\n",
    "                    * self.loss_function(\n",
    "                        densities,\n",
    "                        torch.cat(\n",
    "                            [\n",
    "                                self.training_data[\n",
    "                                    batch_idx + 1 : self.batches[batch_no + 1] + 1, :8\n",
    "                                ],\n",
    "                                self.training_data[\n",
    "                                    batch_idx + 1 : self.batches[batch_no + 1] + 1, [8]\n",
    "                                ],\n",
    "                            ],\n",
    "                            1,\n",
    "                        ),\n",
    "                    ).sum(dim=0)\n",
    "                ).sum()\n",
    "\n",
    "            # Regular loss function\n",
    "            else:\n",
    "                loss = self.loss_function(\n",
    "                    densities,\n",
    "                    self.training_data[batch_idx + 1 : self.batches[batch_no + 1] + 1],\n",
    "                ) / (self.batches[batch_no + 1] - batch_idx)\n",
    "\n",
    "            # Perform a gradient descent step\n",
    "            loss.backward()\n",
    "            self.neural_net.optimizer.step()\n",
    "            self.neural_net.optimizer.zero_grad()\n",
    "            self.current_loss = loss.clone().detach().cpu().numpy().item()\n",
    "            self.current_predictions = torch.tensor(\n",
    "                [\n",
    "                    predicted_parameters.clone().detach().cpu()[self.to_learn[p]]\n",
    "                    * self.scaling_factors[self.to_learn[p]]\n",
    "                    for p in self.to_learn.keys()\n",
    "                ]\n",
    "            )\n",
    "            self._time += 1\n",
    "            self.write_data()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec9bea44601da1a4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def ode(pars):\n",
    "    beta0 = pars[0]\n",
    "    beta2 = pars[1]\n",
    "    beta3 = pars[2]\n",
    "    gamma = pars[3]\n",
    "    yota = pars[4]\n",
    "    \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d418aa3d0ba7ac9a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def neuri(pars, comps):\n",
    "    neural = nn\n",
    "    beta0 = pars[0]\n",
    "    beta2 = pars[1]\n",
    "    beta3 = pars[2]\n",
    "    gamma = pars[3]\n",
    "    yota = pars[4]\n",
    "    \n",
    "    beta = beta0 + beta2 * weathermob[0]**2 + beta3 * weathermob[0]**2 * weathermob[1]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42b0f9bfee3ce840"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
